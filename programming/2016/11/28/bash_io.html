<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Bash IO: handling multiple output files - Genomics in B♭</title>
    <meta name="description" content="Biology, programming, music.">
    <link rel="stylesheet" href="/css/picnic.min.css">
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/pygments.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  </head>

  <body>
    
      <header>
      <nav>
        <a href="/" class="brand">
            <img src="/img/mwhamgenomics.png" width="20" height="20" />
            Genomics in B<span class="music">♭</span>
        </a>

        <input id="minimenu" type="checkbox" class="show">
        <label for="minimenu" class="burger pseudo button"><img class="icon_large" src="/img/menu.svg" /></label>

        <div class="menu">
          <a href="/archive">All posts</a>
          <a href="/bioinformatics">Bioinformatics</a>
          <a href="/programming">Programming</a>
          <a href="/music">Music</a>
          <span class="divider"></span>
          <a href="/tags">Tags</a>
          <a href="/linux">Linux reference</a>
        </div>

      </nav>
    </header>
    

    <div class="flex one two-800">
      <div class="three-fourth-800">
<h2>Bash IO: handling multiple output files</h2>

<p>28 Nov 2016</p>



<p>Category: <a href="/programming">programming</a></p>



<p>Tags: <span>
  
    <a href="/tags#bash">bash</a>, 
  
    <a href="/tags#linux">linux</a>
  
</span></p>


<p>In the course of developing
<a href="https://github.com/EdinburghGenomics/Fastq-Filterer">a C tool for filtering paired-end fastq files</a>, we ran into a
performance problem. C is capable of performing logic and processing data extremely quickly, however, we encountered a
bottleneck at the point of re-compressing output data. This was because we were compressing in a single thread using C's
<code>zlib</code> library. We eventually decided to remove integrated output data compression, and compress instead via an external
tool.</p>
<p>The script we developed is fairly simple to use: the two input fastqs (R1 and R2) are passed as <code>i1</code> and <code>i2</code>, the
corresponding (filtered and uncompressed) output fastqs are specified as <code>o1</code> and <code>o2</code>, and a length threshold at which
to trim out a read pair is passed as <code>threshold</code>:</p>
<pre><code>[mwhamgenomics]$ ./fastq_filterer --i1 r1.fastq.gz --i2 r2.fastq.gz --o1 filtered_r1.fastq --o2 filtered_r2.fastq --threshold 36
</code></pre>
<p>Now to compress the output data. There is a Linux tool called <a href="https://github.com/madler/pigz">Pigz</a> that solves our
performance problem, since it allows compression of data in multiple threads. The only problem we have to solve, then,
is how to link this in to our program (ideally we would do this via pipes without writing any intermediate files to
disk).</p>
<p>If we were only dealing with one output fastq, we'd just be able to pipe it into pigz via <code>stdout</code>, however in this
case, there are two. We could use stdout and stderr, but what if something goes wrong and the filterer has to print an
error message? Fortunately, Bash has solutions for piping multiple channels of data between programs.</p>
<h2>Named pipes</h2>
<p>Named pipes use the <code>mkfifo</code> command, which creates what appears to be a file on your filesystem. However, this is more
like a Python <code>StringIO</code> object, able to stream data instead of writing it to disk. Having created these named pipes, we
can now use parallel commands to stream data to/from them concurrently:</p>
<pre><code>mkfifo intermediate_r1.fastq
mkfifo intermediate_r2.fastq

./fastq_filterer --i1 r1.fastq.gz --i2 r2.fastq.gz --o1 intermediate_r1.fastq --o2 intermediate_r2.fastq --threshold 36 &amp;
pigz -c intermediate_r1.fastq &gt; r1_filtered.fastq.gz &amp;
pigz -c intermediate_r2.fastq &gt; r2_filtered.fastq.gz

rm intermediate_r1.fastq intermediate_r2.fastq
</code></pre>
<p>Here, we have the filterer write its output to named pipes which then pass data to Pigz for compression. Note the use of
<code>&amp;</code> in the first two main commands, and the absence of <code>&amp;</code> in the third. Having created the named pipes, we want to
start up three commands in parallel: one writing to them, and the other two reading them and passing to pigz. It's also
important to delete the named pipes afterwards. This, then, is a very useful and powerful construct, if a little
verbose.</p>
<h2>Process substitution</h2>
<p>Process substitution is an extension of named pipes, and allows a more elegant syntax. Here, you create expressions that
pretend to be files - they can be opened and written to, but underneath, they are Linux pipes. It's probably easiest to
describe how they're used with an example:</p>
<pre><code>set -o pipefail
./filter_fastq_files --i1 r1.fastq.gz --i2 r2.fastq.gz --o1 &gt;(pigz -c &gt; r1_filtered.fastq.gz) --o2 &gt;(pigz -c &gt; r2_filtered.fastq.gz) --threshold 36
</code></pre>
<p>Process substitutions are created with the syntax <code>&gt;(...)</code> if you're writing to them, or <code>&lt;(...)</code> if you're reading. The
filterer opens and writes to them as if they were files, however the <code>(...)</code> allows you to specify some processing to
apply to the data - in this case, piping into pigz.</p>
<p>We encountered a couple of difficulties with process substitution. Firstly, it's recommended to use <code>set -o pipefail</code> to
ensure that the whole pipeline returns a non-zero exit status if one command fails. Secondly, when I tried to put this
into a script to run on Slurm, I found that Slurm does not like process substitutions. It was, however, able to use
named pipes, so we were still able to solve the problem.</p>
<p>Even with complex IO problems, Bash has powerful functionality for streaming data between commands. Using this, we were
able to link one of our own scripts to multithreaded data compression, significantly improving the speed of our pipeline
while avoiding writing too many intermediate files to disk.</p>

<h3>External links</h3>


  <ul>
    
      <li>
        EGCG Fastq-Filterer: <a href="https://github.com/EdinburghGenomics/Fastq-Filterer">https://github.com/EdinburghGenomics/Fastq-Filterer</a>
        <p class="small_font">Source code for the fastq filtering program we developed</p>
      </li>
    
      <li>
        Process substitution on TLDP: <a href="http://www.tldp.org.LDP/abs/html/process-sub.html">http://www.tldp.org.LDP/abs/html/process-sub.html</a>
        <p class="small_font">Notes on process substitution on The Linux Documentation Project</p>
      </li>
    
  </ul>


<p>
  Previous post:
  
    <a class="post-meta" href="/music/2016/10/29/chord_extensions.html">Chord extensions</a>
  
</p>

<p>
  Next post:
  
    <a class="post-meta" href="/programming/2017/04/12/installing_gcc_on_osx.html">Compiling GCC on OSX</a>
  
</p>


</div>

      
      <div class="fourth-800">
        <div class="sidebar">
          <h4>Genomics in B♭</h4>
          <p>
            <i class="fa fa-flask"></i> Biology<br/>
            <i class="fa fa-terminal"></i> Programming<br/>
            <i class="fa fa-music"></i> Music
          </p>

          <p>
            Built with <a href="https://python-markdown.github.io">Python-Markdown</a> and
            <a href="https://jinja.palletsprojects.com">Jinja2</a>. Styled with
            <a href="https://pygments.org">Pygments</a>, <a href="https://picnicss.com">Picnic CSS</a> and
            <a href="https://fontawesome.com">Font Awesome</a>. Hosted on
            <a href="https://pages.github.com">GitHub Pages</a>.
          </p>
          <a href="https://www.github.com/mwhamgenomics"><i class="fa fa-github"></i> mwhamgenomics</a>
        </div>
      </div>
      

    </div>

    
    <footer>
        <p>© http://github.com/mwhamgenomics</p>
    </footer>
    

  </body>

</html>